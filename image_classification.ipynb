{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_from_database import DownloadTables\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "from alive_progress import alive_bar\n",
    "from torchvision.transforms import ToTensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData:\n",
    "    \"\"\"\n",
    "    Class to process images for a classifcation model.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        See help(PrepareData)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def retrieve_dataframes(self, credentials_location:str=None):\n",
    "        \"\"\"\n",
    "        Retrives the Dataframes and assigns them as instance variables.\n",
    "\n",
    "        Attributes:\n",
    "            crediential_location(str): Optionally, the location of the yaml file containing the database credentials. It should contain DATABASE_TYPE, DBAPI, ENDPOINT, USER, PASSWORD, PORT, DATABASE\n",
    "        \n",
    "        Returns:\n",
    "            product_df(DataFrame): The clean DataFrame from the products tables. \n",
    "            image_df (DataFrame): The clean DataFrame from the images table. \n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        if credentials_location != None:\n",
    "            downloader = DownloadTables(credentials_location)\n",
    "            downloader.download_table(\"images\")\n",
    "            downloader.download_table(\"products\")\n",
    "        \n",
    "        #TODO add in a step to clean the products table if needed\n",
    "    \n",
    "        product_df = pd.read_json(\"data/products_table_clean.json\")\n",
    "        image_df = pd.read_json(\"data/images_table.json\")\n",
    "        \n",
    "        self.product_df = product_df\n",
    "        self.image_df = image_df\n",
    "\n",
    "        return product_df, image_df\n",
    "\n",
    "    def retrieve_image_category(self, image_name:str) -> str:\n",
    "        \"\"\"\n",
    "        Given the ID of an image, retrieves the category. \n",
    "\n",
    "        Attributes:\n",
    "            image_name(str): the name of when \n",
    "        \"\"\"\n",
    "        image_row = self.image_df.loc[self.image_df[\"id\"] == image_name]\n",
    "        product_id = image_row.iloc[0][\"product_id\"]\n",
    "\n",
    "        product_row = self.product_df.loc[self.product_df[\"id\"] == product_id]\n",
    "        category = product_row.iloc[0][\"main_category\"]\n",
    "        \n",
    "        return category\n",
    "\n",
    "    @staticmethod\n",
    "    def image_to_array(image_location):\n",
    "        \"\"\"\n",
    "        Converts an image to a numpy array, \n",
    "        \"\"\"\n",
    "        image = Image.open(image_location)\n",
    "        image = ToTensor()(image)\n",
    "        image = torch.flatten(image)\n",
    "        return image.numpy()\n",
    "\n",
    "    \n",
    "    def convert_to_image_array(self, image):\n",
    "        image_category = self.retrieve_image_category(image)\n",
    "        image_location = images_folder +\"/\"+ image +\".jpg\"\n",
    "        image_array = self.image_to_array(image_location)\n",
    "        return image_array, image_category\n",
    "\n",
    "    \n",
    "    def create_dict_of_categories(self): \n",
    "        categories = set(self.product_df[\"main_category\"]) \n",
    "        categories_dict = {k: v for v, k in enumerate(categories)} \n",
    "        self.categories_dict = categories_dict\n",
    "        return categories_dict\n",
    "\n",
    "\n",
    "    def form_arrays(self, image_size:int, n:int = None):\n",
    "        \n",
    "        images = list(image_df[\"id\"])\n",
    "\n",
    "        # if the number of images hasn't been specified, look at all images\n",
    "        if n == None:\n",
    "            n = len(images)\n",
    "\n",
    "        # sets up the arrays\n",
    "        # array_size is based on a square image with three channels\n",
    "        array_size = (image_size**2)*3\n",
    "        X = np.zeros((n,array_size))\n",
    "        y = np.zeros(n)\n",
    "\n",
    "        \n",
    "        #set up a dictionary assigning categories to integers\n",
    "        pipeline.create_dict_of_categories()\n",
    "        \n",
    "\n",
    "        for index in range(n):\n",
    "            image = images[index]\n",
    "            try:\n",
    "                features, label = pipeline.convert_to_image_array(image)\n",
    "                X[index, :] = features\n",
    "                y[index] = self.categories_dict[label]\n",
    "\n",
    "            except:\n",
    "                #TODO deal with this because it might be affecting the model\n",
    "                X[index, :] = np.zeros(array_size)\n",
    "                y[index] = 0\n",
    "\n",
    "\n",
    "        return X, y\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/images_table.json  is already downloaded, skipping\n",
      "data/products_table.json  is already downloaded, skipping\n"
     ]
    }
   ],
   "source": [
    "credentials_location = \".gitignore/credentials_for_marketplace.yml\"\n",
    "\n",
    "pipeline = PrepareData()\n",
    "product_df, image_df = pipeline.retrieve_dataframes(credentials_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images_folder = \"data/cleaned_images_64\"\n",
    "image_size = 64\n",
    "\n",
    "\n",
    "X, y = pipeline.form_arrays(image_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16699722332407774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amy/miniconda3/envs/rec_ranking/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LogisticRegression(max_iter=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "score = model.score(X_test, y_test)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f53e2f1528c4ea6ed94c8d97c2244a74e89c0ff201e49f44e466841bc02a24f6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('rec_ranking')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
